#!/usr/bin/env bash
set -Eeo pipefail

CI=${CI:-false}
HAB_ORIGIN=${HAB_ORIGIN:-chef}
TIMEOUT=$((${BUILDKITE_TIMEOUT:-32} - 2))

source_dir=$(cd "$(dirname "${BASH_SOURCE[0]}" )" && pwd)
source "${source_dir}/helpers/log.sh"
source "${source_dir}/base.sh"
source "$1"
BUILDKITE_JOB_ID="${BUILDKITE_JOB_ID:-"${test_build_slug}"}"
complete_commandline="$0 $*"

test_commandline() {
    local env_vars
    if [[ -n "$VAULT_UTIL_SECRETS" ]]; then
        echo "    # When on the Chef VPN"
        echo "    CHEF_USERNAME=yourusername scripts/get_secrets.sh"
        echo "    export VAULT_ADDR=https://vault.chef.co:8200"
       # jq -r 'to_entries | map("    export " + .key + "=$(vault kv get -field=" + .value.field + " " + .value.path + ")")[]' <<<"$VAULT_UTIL_SECRETS"
    fi

    # Reproduce a select set of environment variables that influence
    # the tests.
    env_vars=$(set +o pipefail; env | grep -E '^(FLAKY|IAM)=' | tr '\n' ' ')
    echo "    $env_vars $complete_commandline"
}

cleanup() {
    if [ ${#test_external_services[@]} -ne 0 ]; then
        log_info "Cleaning Up External Services"

        for external_service in "${test_external_services[@]}"
        do
            log_info "Dumping logs for $external_service"
            ${external_service}_dump_logs
            log_info "Stopping $external_service"
            ${external_service}_teardown
        done

        destroy_services_config_path
    fi
    echo "Attempting to clean up docker container $test_container_name"
    docker stop "$test_container_name"
    if [[ "${test_proxy}" = "true" ]]; then
        echo "Attempting to clean up docker container $test_proxy_container_name"
        docker stop "$test_proxy_container_name"
        echo "Attempting to clean up docker network $test_proxy_internal_network_name"
        docker network rm "$test_proxy_internal_network_name"
    fi
    echo "------------------------------------------------------------"
    echo "To run this test locally:"
    echo ""
    test_commandline
    echo ""
    echo "------------------------------------------------------------"
}

cleanup_ci() {
    # We condition this on CI so that in CI we always cleanup, even on failure
    if [[ "$CI" = "true" ]]; then
        cleanup
    else
        echo "Not cleaning up because CI=$CI"
    fi
}

if [[ "$CI" = "true" ]]; then
    echo "===== PRE-TEST HOST HEALTH REPORT ====="
    echo "docker ps"
    docker ps
    echo "free -m"
    free -m
    echo "uptime"
    uptime
    echo "======================================="
else
    # In CI we expect the images to have already pulled the latest docker
    # layers, but we can't rely on that for local machines. Here we'll make sure
    # we start containers with the latest images.
    docker pull chefes/centos-systemd:latest
    docker pull chefes/a2-integration:latest
fi

workdir="/go/src/github.com/chef/automate"
CURR_DIR=$(pwd)

docker_run_args=(
        "--detach"
        "--env" "CI"
        "--env" "HOST_PWD=$CURR_DIR"
        "--env" "HAB_ORIGIN=$HAB_ORIGIN"
        "--env" "HAB_STUDIO_SUP=false"
        "--env" "HAB_NONINTERACTIVE=true"
        "--env" "CONTAINER_HOSTNAME=${test_container_name}"
        "--env" "test_notifications_endpoint=${test_notifications_endpoint}"
        "--env" "IAM"
        "--env" "FLAKY"
        "--env" "A2_LICENSE"
        "--env" "CYPRESS_AUTOMATE_ACCEPTANCE_TARGET_HOST"
        "--env" "CYPRESS_AUTOMATE_ACCEPTANCE_TARGET_USER"
        "--env" "CYPRESS_AUTOMATE_ACCEPTANCE_TARGET_KEY"
        "--env" "CYPRESS_AUTOMATE_INFRA_ADMIN_KEY"
        "--env" "AWS_ACCESS_KEY_ID"
        "--env" "AWS_SECRET_ACCESS_KEY"
        "--env" "AWS_SESSION_TOKEN"
        "--env" "GOOGLE_APPLICATION_JSON"
        "--env" "test_container_name=${test_container_name}"
        "--env" "test_build_slug=${test_build_slug}"
        "--env" "test_proxy_container_name=${test_proxy_container_name}"
        "--env" "test_proxy_internal_network_name=${test_proxy_internal_network_name}"
        "--hostname" "$test_container_name"
        "--interactive"
        "--name" "$test_container_name"
        "--privileged"
        "--rm"
        "--tmpfs=/tmp:rw,noexec,nosuid"
        "--tmpfs=/var/tmp:rw,noexec,nosuid"
        "--tmpfs=/dev/shm:rw,noexec,nosuid"
        "--tty"
        "--volume" "$CURR_DIR:$workdir"
        # I have no idea why our container doesn't like it when
        # we try to mount docker.sock in the default place. Removing
        # the privileged flag makes it work.
        "-v" "/var/run/docker.sock:/docker.sock"
        "--env" "DOCKER_HOST=unix:///docker.sock"
        "--workdir" "$workdir"
)

if [ $CI = "true" ]
then
    buildkite_agent=$(command -v buildkite-agent)
    docker_run_args+=(
        "--env" "BUILDKITE_JOB_ID"
        "--env" "BUILDKITE_BUILD_ID"
        "--env" "BUILDKITE_AGENT_ACCESS_TOKEN"
        "--env" "BUILDKITE"
        "--volume" "$buildkite_agent:/usr/bin/buildkite-agent"
        "--label" "buildkitejob=$BUILDKITE_JOB_ID"
    )
fi


trap cleanup_ci EXIT


if [[ "${test_proxy}" = "true" ]]; then
    mkdir -p logs
    log_info "Creating proxy network"
    # Create a network with no internet access.
    # Automate will connect to this network
    docker network create "$test_proxy_internal_network_name" \
        --internal \
        --label "buildkitejob=$BUILDKITE_JOB_ID"
    docker_run_args+=(
        "--network" "${test_proxy_internal_network_name}"
    )

    # Run squid and connect it to the network without internet
    # access so that automate can access it.
    docker run \
        --detach --rm \
        --name "${test_proxy_container_name}" \
        --hostname "${test_proxy_container_name}" \
        --label "buildkitejob=$BUILDKITE_JOB_ID" \
        --network "${test_proxy_internal_network_name}" \
        --volume "$PWD/logs:/var/log/squid" \
        --volume "$PWD/integration/squid.conf:/etc/squid/squid.conf" \
        $(docker build -q $PWD/integration/docker-squid)

    # Connect the proxy container to the default bridge network
    # which has access to the internet
    docker network connect bridge "${test_proxy_container_name}"
fi

if [ ${#test_external_services[@]} -ne 0 ]; then
    log_info "Start External Services"
    source integration/services/common.sh
    create_services_config_path

    docker_run_args+=(
        "--volume" "$SERVICES_CONFIG_PATH:/services"
    )

    for external_service in "${test_external_services[@]}"
    do
        log_info "Starting $external_service"
        source integration/services/$external_service/init.sh
        ${external_service}_setup
    done
fi

if [ ! -z "$EXTERNAL_ONLY" ]; then
    echo "Started external services... exiting"
    exit 0
fi

echo "${docker_run_args[*]}"

docker run "${docker_run_args[@]}" chefes/a2-integration:latest

if [ $CI != "true" ]; then
    secret_origin_key=$(HAB_LICENSE=accept-no-persist hab origin key export --type secret "$HAB_ORIGIN")
    docker exec -e SEC_KEY="$secret_origin_key" -e HAB_LICENSE="accept-no-persist" "$test_container_name" bash -c 'echo "$SEC_KEY" | hab origin key import'

    public_origin_key=$(HAB_LICENSE=accept-no-persist hab origin key export --type public "$HAB_ORIGIN")
    docker exec -e SEC_KEY="$public_origin_key" -e HAB_LICENSE="accept-no-persist" "$test_container_name" bash -c 'echo "$SEC_KEY" | hab origin key import'
fi

echo "Running the docker. Will timeout after $TIMEOUT mins"
docker exec -e TEST_PATH=$1 "$test_container_name" timeout "${TIMEOUT}m" bash -c 'source ./integration/base.sh; __run_test $TEST_PATH'
